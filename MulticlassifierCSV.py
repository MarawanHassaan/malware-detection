##########################################################Imports#####################################################################################
import numpy as np
import json_lines
import pandas as pd
import random
from sklearn.feature_extraction.text import *
import pickle
import csv


from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import *
from sklearn.naive_bayes import *
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel


'''
instructions=[] #X matrix that will be passed to count vectorizer
labels=[] #Y array which contains the target values



####################################################Preparing the dataset###############################################################################
print("Opening the file ...")
with open('train_dataset.jsonl', 'rb') as f: # opening file in binary(rb) mode

   print("Preparing the dataset ...") 
   for item in json_lines.reader(f): #We move on each line sequentially. item means one line
       inst=item['instructions'] #inst will point to the array of assembly instructions
       label=item['compiler'] #label captures the optimization value high or low
       labels.append(label)
       list_inst=[]
       for each_inst in inst: #each_inst will point to one element in the list of instructions as "push r12", "push rbp", "push rbx",
           list_inst.append(each_inst.split(' ',1)[0]) #Split the string after each 'space' and consider only the mnemonic part which is the first one aka [0]
           to_be_added=' '.join(list_inst)
       instructions.append(to_be_added) #instructions[0] will be {push push push test mov mov mov call ........}



####################################################Extracting the features#############################################################################
#For the lists to be passed to Sklearn, they need to be converted to numpy arrays    
instructions = np.array(instructions) 
labels = np.array(labels)

#Building the feature vector using counter vectorizer method with n-grams starting from 1 and up to 3
vectorizer = TfidfVectorizer(ngram_range=(1, 3))   

X_all = vectorizer.fit_transform(instructions) #X_all will be a matrix where each line has the same length
y_all = labels

print(X_all.shape)
print(y_all.shape)

#Save the vectorizer to the disk to be used on the blind dataset
filename1 = 'finalized_vectorizer_multiclass.sav'
pickle.dump(vectorizer, open(filename1, 'wb'))
print('Vectorizer dumped')

####################################################Feature selection and model creation################################################################

X_train, X_test, y_train, y_test = train_test_split(X_all, y_all,test_size=0.2, random_state=15)
id = random.randrange(0,X_train.shape[0])
print('%d ' %(id))
print("Train: %d - Test: %d" %(X_train.shape[0],X_test.shape[0]))
    #train_linear_SVC(X_train, X_test, y_train, y_test)
    #train_logistic_regression(X_train, X_test, y_train, y_test)
    #train_multinomialNB(X_train, X_test, y_train, y_test)
    #train_decision_tree(X_train, X_test, y_train, y_test)
model = RandomForestClassifier(n_estimators=100,random_state=0).fit(X_train, y_train)
print('Random Forest created')
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test,y_pred))


#Save the model to disk to be used on the blind dataset
filename2 = 'finalized_model_multiclass.sav'
pickle.dump(model, open(filename2, 'wb'))
print('Model dumped')

'''
#######################################################Blind Dataset####################################################################################

#################################################Vectorizer and Model Retrieval#########################################################################

#Retrieve the model and the vectorizer from the disk
filename1 = 'finalized_vectorizer_multiclass.sav'
vectorizer = pickle.load(open(filename1, 'rb'))
print('Vectorizer loaded')
filename2 = 'finalized_model_multiclass.sav'
model = pickle.load(open(filename2, 'rb'))
print('Model loaded')


instructions=[]
print('Preparing the blind dataset for testing ... ')
print("Opening the test file ...")
with open('test_dataset_blind.jsonl', 'rb') as f: # opening file in binary(rb) mode

   print("Preparing the test dataset ...")
   for item in json_lines.reader(f):  # We move on each line sequentially. item means one line
       inst = item['instructions']
       list_inst = []
       for each_inst in inst:
           list_inst.append(each_inst.split(' ', 1)[0])
           to_be_added = ' '.join(list_inst)
       instructions.append(to_be_added)  # instructions[0] will be {push push push test mov mov mov call ........}



####################################################Extracting the features#############################################################################
#For the lists to be passed to Sklearn, they need to be converted to numpy arrays    
instructions = np.array(instructions) 


X_all = vectorizer.transform(instructions) #X_all will be a matrix where each line has the same length
print(X_all.shape)
y_pred= model.predict(X_all)


#Counter is initialized for each compiler to keep track of their number in the blind dataset
count_gcc=0
count_icc=0
count_clang=0

for y in y_pred:
    if y == 'gcc':
        count_gcc= count_gcc +1
    elif y=='clang':
        count_clang= count_clang+1
    elif y=='icc':
        count_icc= count_icc+1

print(count_gcc)
print(count_icc)
print(count_clang)

####################################################Writing the prediction in CSV#######################################################################
with open('test_multiclass.csv', mode='w' ,newline='') as f:
    writer = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    for y in y_pred:
        writer.writerow([y])





    
