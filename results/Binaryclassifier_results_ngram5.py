Python 3.7.4 (tags/v3.7.4:e09359112e, Jul  8 2019, 20:34:20) [MSC v.1916 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license()" for more information.
>>> 
= RESTART: C:\Users\maraw\OneDrive\Desktop\ML homework 1\Binaryclassifier.py =
Opening the file ...
Preparing the dataset ...
(24152,)
(24152,)
(24152, 852544)
(24152,)
logr_features_weights

Warning (from warnings module):
  File "C:\Users\maraw\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\logistic.py", line 432
    FutureWarning)
FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.

Warning (from warnings module):
  File "C:\Users\maraw\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py", line 929
    "the number of iterations.", ConvergenceWarning)
ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
(24152, 190066)
Random element selection is chosen at 11969 
Train: 19321 - Test: 4831
LinearSVC model created
[[1992  331]
 [ 254 2254]]
              precision    recall  f1-score   support

           H       0.89      0.86      0.87      2323
           L       0.87      0.90      0.89      2508

    accuracy                           0.88      4831
   macro avg       0.88      0.88      0.88      4831
weighted avg       0.88      0.88      0.88      4831

0.8789070585800042
Logistic Regression model created
[[1935  388]
 [ 272 2236]]
              precision    recall  f1-score   support

           H       0.88      0.83      0.85      2323
           L       0.85      0.89      0.87      2508

    accuracy                           0.86      4831
   macro avg       0.86      0.86      0.86      4831
weighted avg       0.86      0.86      0.86      4831

0.8633823225005175
Multinomial Naive Bayes model created
[[2133  190]
 [ 563 1945]]
              precision    recall  f1-score   support

           H       0.79      0.92      0.85      2323
           L       0.91      0.78      0.84      2508

    accuracy                           0.84      4831
   macro avg       0.85      0.85      0.84      4831
weighted avg       0.85      0.84      0.84      4831

0.8441316497619541
Decision Tree created
[[1977  346]
 [ 322 2186]]
              precision    recall  f1-score   support

           H       0.86      0.85      0.86      2323
           L       0.86      0.87      0.87      2508

    accuracy                           0.86      4831
   macro avg       0.86      0.86      0.86      4831
weighted avg       0.86      0.86      0.86      4831

0.861726350652039
Random Forest created
[[2138  185]
 [ 310 2198]]
              precision    recall  f1-score   support

           H       0.87      0.92      0.90      2323
           L       0.92      0.88      0.90      2508

    accuracy                           0.90      4831
   macro avg       0.90      0.90      0.90      4831
weighted avg       0.90      0.90      0.90      4831

0.8975367418753881
svm_features_weights

Warning (from warnings module):
  File "C:\Users\maraw\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\svm\base.py", line 929
    "the number of iterations.", ConvergenceWarning)
ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
(24152, 138260)
Random element selection is chosen at 4786 
Train: 19321 - Test: 4831
LinearSVC model created
[[2024  299]
 [ 283 2225]]
              precision    recall  f1-score   support

           H       0.88      0.87      0.87      2323
           L       0.88      0.89      0.88      2508

    accuracy                           0.88      4831
   macro avg       0.88      0.88      0.88      4831
weighted avg       0.88      0.88      0.88      4831

0.8795280480231836

Warning (from warnings module):
  File "C:\Users\maraw\AppData\Local\Programs\Python\Python37\lib\site-packages\sklearn\linear_model\logistic.py", line 432
    FutureWarning)
FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.
Logistic Regression model created
[[1892  431]
 [ 295 2213]]
              precision    recall  f1-score   support

           H       0.87      0.81      0.84      2323
           L       0.84      0.88      0.86      2508

    accuracy                           0.85      4831
   macro avg       0.85      0.85      0.85      4831
weighted avg       0.85      0.85      0.85      4831

0.8497205547505693
Multinomial Naive Bayes model created
[[2107  216]
 [ 658 1850]]
              precision    recall  f1-score   support

           H       0.76      0.91      0.83      2323
           L       0.90      0.74      0.81      2508

    accuracy                           0.82      4831
   macro avg       0.83      0.82      0.82      4831
weighted avg       0.83      0.82      0.82      4831

0.8190850755537156
Decision Tree created
[[1972  351]
 [ 316 2192]]
              precision    recall  f1-score   support

           H       0.86      0.85      0.86      2323
           L       0.86      0.87      0.87      2508

    accuracy                           0.86      4831
   macro avg       0.86      0.86      0.86      4831
weighted avg       0.86      0.86      0.86      4831

0.8619333471330988
Random Forest created
[[2140  183]
 [ 310 2198]]
              precision    recall  f1-score   support

           H       0.87      0.92      0.90      2323
           L       0.92      0.88      0.90      2508

    accuracy                           0.90      4831
   macro avg       0.90      0.90      0.90      4831
weighted avg       0.90      0.90      0.90      4831

0.8979507348375078
>>> 
