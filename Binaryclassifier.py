##########################################################Imports#####################################################################################
import numpy as np
import json_lines
import pandas as pd
import random
from Binaryclassifier_train import *

 
instructionslow=[] #X matrix that will be passed to count vectorizer
labelslow=[] #Y array which contains the target values
instructionshigh=[] #X matrix that will be passed to count vectorizer
labelshigh=[]


####################################################Preparing the dataset###############################################################################
print("Opening the file ...")
with open('train_dataset.jsonl', 'rb') as f: # opening file in binary(rb) mode

   print("Preparing the dataset ...") 
   for item in json_lines.reader(f): #We move on each line sequentially. item means one line which is JSON object
       inst=item['instructions'] #inst will point to the array of assembly instructions
       label=item['opt'] #label captures the optimization value (high or low)
       #Since the number of low in the dataset is greater than than of the high,the first ones of the Low are considered to make the dataset balanced
       if label=='L':
           labelslow.append(label)
           list_inst=[]
           for each_inst in inst: #each_inst will point to one element in the list of instructions as "push r12", "push rbp", "push rbx",
               list_inst.append(each_inst.split(' ',1)[0]) #Split the string after each 'space' and consider only the mnemonic part which is the first one aka [0] 
               to_be_added=' '.join(list_inst) #Add each to the list to_be_added as a part of a string
           instructionslow.append(to_be_added) #instructions[0] will be {push push push test mov mov mov call ........}


       elif label == 'H': #All entries with optimization value equals to high are taken since they are the minority
            labelshigh.append(label)
            list_inst=[]
            for each_inst in inst:
                list_inst.append(each_inst.split(' ',1)[0])
                to_be_added=' '.join(list_inst)
            instructionshigh.append(to_be_added)


needed_low=random.choices(instructionslow, k=12076)

instructions= instructionshigh+needed_low
labels=labelshigh+labelslow[0:12076]

####################################################Extracting the features#############################################################################
#For the lists to be passed to Sklearn, they need to be converted to numpy arrays    
instructions = np.array(instructions) 
labels = np.array(labels)
print(instructions.shape)
print(labels.shape)
#Building the feature vector using counter vectorizer method with n-grams starting from 1 and up to 3
vectorizer = CountVectorizer(ngram_range=(1, 3))
#vectorizer = CountVectorizer(ngram_range=(1, 5))

X_all = vectorizer.fit_transform(instructions) #X_all will be a matrix where each line has the same length
y_all = labels

print(X_all.shape)
print(y_all.shape)



#####################################################Feature Selection and training######################################################################

#To reduce the number of features in the feature vector, Two techniques are applied (Logistic regression, Linear SVM)
features_selection = [logr_features_weights, svm_features_weights]
for reduce_features in features_selection:
    print(reduce_features.__name__)
    new_features_vectors = reduce_features(X_all,y_all) #New feature matrix is returned containing only the most important features
    print(new_features_vectors.shape)
    X_train, X_test, y_train, y_test = train_test_split(new_features_vectors, y_all,test_size=0.2, random_state=15)
    id = random.randrange(0,X_train.shape[0])
    print('Random element selection is chosen at %d ' %(id))
    print("Train: %d - Test: %d" %(X_train.shape[0],X_test.shape[0]))
    train_linear_SVC(X_train, X_test, y_train, y_test)
    train_logistic_regression(X_train, X_test, y_train, y_test)
    train_multinomialNB(X_train, X_test, y_train, y_test)
    train_decision_tree(X_train, X_test, y_train, y_test)
    train_random_forest(X_train, X_test, y_train, y_test)














    
