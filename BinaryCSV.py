##########################################################Imports#####################################################################################
import numpy as np
import json_lines
import pandas as pd
import random
from Binaryclassifier_train import *

import pickle
import csv


from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import *
from sklearn.naive_bayes import *
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
'''
number =0 
instructionslow=[] #X matrix that will be passed to count vectorizer
labelslow=[] #Y array which contains the target values
instructionshigh=[] #X matrix that will be passed to count vectorizer
labelshigh=[]


####################################################Preparing the dataset###############################################################################
print("Opening the file ...")
with open('train_dataset.jsonl', 'rb') as f: # opening file in binary(rb) mode

   print("Preparing the dataset ...") 
   for item in json_lines.reader(f): #We move on each line sequentially. item means one line
       inst=item['instructions'] #inst will point to the array of assembly instructions
       label=item['opt'] #label captures the optimization value high or low
       #Since the number of low in the dataset is greater than than of the high I am only taking the first ones that make the dataset balanced
       if label=='L':  
           number = number +1
           labelslow.append(label)
           list_inst=[]
           for each_inst in inst: #each_inst will point to one element in the list of instructions as "push r12", "push rbp", "push rbx",
               list_inst.append(each_inst.split(' ',1)[0]) #Split the string after each 'space' and consider only the mnemonic part which is the first one aka [0] 
               to_be_added=' '.join(list_inst) #Add each to the list to_be_added as a part of a string
           instructionslow.append(to_be_added) #instructions[0] will be {push push push test mov mov mov call ........}


       elif label == 'H':
            labelshigh.append(label)
            list_inst=[]
            for each_inst in inst:
                list_inst.append(each_inst.split(' ',1)[0])
                to_be_added=' '.join(list_inst)
            instructionshigh.append(to_be_added)
needed_low=random.choices(instructionslow, k=12076)

instructions= instructionshigh+needed_low
labels=labelshigh+labelslow[0:12076]
#for x in range(len(labelslow)):
    #all_low.append([instructionslow[x],labelslow[x]])


####################################################Extracting the features#############################################################################
#For the lists to be passed to Sklearn, they need to be converted to numpy arrays    
instructions = np.array(instructions) 
labels = np.array(labels)

#Building the feature vector using counter vectorizer method with n-grams starting from 1 and up to 3
vectorizer = CountVectorizer(ngram_range=(1, 3))

X_all = vectorizer.fit_transform(instructions) #X_all will be a matrix where each line has the same length
y_all = labels

print(X_all.shape)
print(y_all.shape)

#Save the vectorizer to the disk to be used on the blind dataset
filename1 = 'finalized_vectorizer.sav'
pickle.dump(vectorizer, open(filename1, 'wb'))
print('Vectorizer dumped')

#######################################Feature Selection and learning###############################################



X_train, X_test, y_train, y_test = train_test_split(X_all, y_all,test_size=0.2, random_state=15)
id = random.randrange(0,X_train.shape[0])
print('%d ' %(id))
print("Train: %d - Test: %d" %(X_train.shape[0],X_test.shape[0]))
    #train_linear_SVC(X_train, X_test, y_train, y_test)
    #train_logistic_regression(X_train, X_test, y_train, y_test)
    #train_multinomialNB(X_train, X_test, y_train, y_test)
    #train_decision_tree(X_train, X_test, y_train, y_test)

#Random forest selected based on its good results in the previous secion
model = RandomForestClassifier(n_estimators=100,random_state=0).fit(X_train, y_train)
print('Random Forest created')
y_pred = model.predict(X_test)
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))
print(accuracy_score(y_test,y_pred))


#Save the model to disk to be used on the blind dataset

filename2 = 'finalized_model.sav'
pickle.dump(model, open(filename2, 'wb'))
print('Model dumped')

'''
#######################################################Blind Dataset####################################################################################


#################################################Vectorizer and Model Retrieval#########################################################################


#Retrieve the model and the vectorizer from the drive using the pickle technique 
filename1 = 'finalized_vectorizer.sav'
vectorizer = pickle.load(open(filename1, 'rb'))
print('Vectorizer loaded')
filename2 = 'finalized_model.sav'
model = pickle.load(open(filename2, 'rb'))
print('Model loaded')


instructions=[]
print('Preparing your blind dataset')
print("Opening the test file ...")
with open('test_dataset_blind.jsonl', 'rb') as f: # opening file in binary(rb) mode

   print("Preparing the test dataset ...")
   for item in json_lines.reader(f):  # We move on each line sequentially. item means one line
       inst = item['instructions']
       list_inst = []
       for each_inst in inst:
           list_inst.append(each_inst.split(' ', 1)[0])
           to_be_added = ' '.join(list_inst)
       instructions.append(to_be_added)  # instructions[0] will be {push push push test mov mov mov call ........}



####################################################Extracting the features#############################################################################
#For the lists to be passed to Sklearn, they need to be converted to numpy arrays    
instructions = np.array(instructions) 



X_all = vectorizer.transform(instructions) #X_all will be a matrix where each line has the same length
print(X_all.shape)
y_pred= model.predict(X_all)

#Counting the number of prediction values (High or low)
count_low=0
count_high=0
for y in y_pred:
    if y == 'L':
        count_low= count_low +1
    elif y=='H':
        count_high= count_high+1
print(count_low)
print(count_high)

with open('test_binary.csv', mode='w' ,newline='') as f:
    writer = csv.writer(f, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
    for y in y_pred:
        writer.writerow(y)






    
